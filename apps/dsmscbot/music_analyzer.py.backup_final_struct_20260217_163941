"""
Улучшенный модуль для анализа музыкальных треков
С защитой от ошибок, проверкой удвоения BPM и более точной сегментацией
"""

import os
import librosa
import numpy as np
from mutagen import File
from mutagen.id3 import ID3
from typing import Dict, List, Tuple, Optional
import warnings
import logging

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


class MusicAnalyzer:
    """Класс для анализа музыкальных характеристик трека"""
    
    # Camelot Wheel для гармонического микширования
    CAMELOT_WHEEL = {
        'C': '8B', 'C#': '3B', 'D': '10B', 'D#': '5B', 
        'E': '12B', 'F': '7B', 'F#': '2B', 'G': '9B',
        'G#': '4B', 'A': '11B', 'A#': '6B', 'B': '1B',
        'Cm': '5A', 'C#m': '12A', 'Dm': '7A', 'D#m': '2A',
        'Em': '9A', 'Fm': '4A', 'F#m': '11A', 'Gm': '6A',
        'G#m': '1A', 'Am': '8A', 'A#m': '3A', 'Bm': '10A'
    }
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.filename = os.path.basename(file_path)
        self.y = None
        self.sr = None
        self.loaded = False
        
    def load_audio(self, duration: Optional[int] = None) -> bool:
        """Загрузка аудиофайла с проверкой"""
        try:
            self.y, self.sr = librosa.load(
                self.file_path, 
                sr=22050,
                duration=duration or 90,
                mono=True
            )
            self.loaded = True
            return True
        except Exception as e:
            logger.error(f"Ошибка загрузки {self.filename}: {e}")
            self.loaded = False
            return False
    
    def analyze_bpm(self) -> float:
        """
        Улучшенное определение BPM с проверкой удвоения/половинного темпа
        """
        if not self.loaded:
            logger.warning(f"{self.filename}: аудио не загружено, BPM=0")
            return 0.0
        
        try:
            # Метод 1: Onset strength (основной)
            onset_env = librosa.onset.onset_strength(y=self.y, sr=self.sr)
            tempo_onset = librosa.feature.tempo(onset_envelope=onset_env, sr=self.sr)
            # Извлекаем скаляр из массива
            if isinstance(tempo_onset, np.ndarray):
                tempo_onset = float(tempo_onset[0])
            else:
                tempo_onset = float(tempo_onset)
            
            # Метод 2: Beat tracking для верификации
            tempo_beat, _ = librosa.beat.beat_track(y=self.y, sr=self.sr)
            if isinstance(tempo_beat, np.ndarray):
                tempo_beat = float(tempo_beat[0])
            else:
                tempo_beat = float(tempo_beat)
            
            # Метод 3: Autocorrelation для медленных темпов
            ac = librosa.autocorrelate(onset_env, max_size=self.sr // 2)
            peaks = librosa.util.peak_pick(ac, pre_max=3, post_max=3, pre_avg=3, post_avg=5, delta=0.5, wait=10)
            
            if len(peaks) > 0 and peaks[0] > 0:
                tempo_ac = 60 * self.sr / peaks[0]
            else:
                tempo_ac = tempo_onset
            
            # Усредняем результаты с весами
            tempo = (tempo_onset * 0.5 + tempo_beat * 0.3 + tempo_ac * 0.2)
            
            # Проверяем диапазон
            if tempo < 30 or tempo > 300:
                tempo = tempo_onset
            
            # Проверка на удвоение/половинный темп
            tempo = self._check_double_half(tempo, onset_env)
            
            return round(float(tempo), 2)
            
        except Exception as e:
            logger.error(f"Ошибка анализа BPM для {self.filename}: {e}")
            return 0.0
    
    def _check_double_half(self, tempo: float, onset_env: np.ndarray) -> float:
        """Проверка, не является ли текущий темп удвоением или половиной реального"""
        if 65 < tempo < 95:
            # Возможен half-time, проверим double
            tempo_double = tempo * 2
            ac = librosa.autocorrelate(onset_env, max_size=self.sr // 2)
            lag_double = int(60 * self.sr / tempo_double)
            if lag_double < len(ac):
                peak_double = ac[lag_double]
                lag_current = int(60 * self.sr / tempo)
                if lag_current < len(ac):
                    peak_current = ac[lag_current]
                    if peak_double > peak_current * 1.2:
                        logger.debug(f"BPM {tempo} -> вероятно half-time, реальный {tempo_double}")
                        return tempo_double
        elif 130 < tempo < 190:
            # Возможен double-time, проверим half
            tempo_half = tempo / 2
            ac = librosa.autocorrelate(onset_env, max_size=self.sr // 2)
            lag_half = int(60 * self.sr / tempo_half)
            if lag_half < len(ac):
                peak_half = ac[lag_half]
                lag_current = int(60 * self.sr / tempo)
                if lag_current < len(ac):
                    peak_current = ac[lag_current]
                    if peak_half > peak_current * 1.2:
                        logger.debug(f"BPM {tempo} -> вероятно double-time, реальный {tempo_half}")
                        return tempo_half
        return tempo
    
    def analyze_key(self) -> Tuple[str, str]:
        """Определение тональности трека"""
        if not self.loaded:
            return 'Unknown', '?'
        
        try:
            # Используем CQT хромограмму для лучшей точности
            chroma = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)
            
            # Находим преобладающую тональность
            key_idx = np.argmax(np.sum(chroma, axis=1))
            keys = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
            
            # Профили мажора и минора (Krumhansl-Schmuckler)
            major_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])
            minor_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])
            
            # Нормализуем
            major_profile = major_profile / np.sum(major_profile)
            minor_profile = minor_profile / np.sum(minor_profile)
            
            chroma_avg = np.mean(chroma, axis=1)
            chroma_avg = chroma_avg / np.sum(chroma_avg)
            
            # Вычисляем корреляцию
            major_corr = np.corrcoef(chroma_avg, major_profile)[0, 1]
            minor_corr = np.corrcoef(chroma_avg, minor_profile)[0, 1]
            
            key = keys[key_idx]
            if minor_corr > major_corr:
                key += 'm'
            
            camelot = self.CAMELOT_WHEEL.get(key, '?')
            
            return key, camelot
            
        except Exception as e:
            logger.error(f"Ошибка анализа тональности для {self.filename}: {e}")
            return 'Unknown', '?'
    
    def analyze_energy_mood(self) -> Tuple[float, str]:
        """
        Определение энергии/настроения трека
        Возвращает: (energy_value, mood_description)
        """
        if not self.loaded:
            return 0.5, "Unknown"
        
        try:
            # Множественные метрики для точной оценки энергии
            
            # 1. RMS энергия
            rms = librosa.feature.rms(y=self.y)[0]
            rms_energy = np.mean(rms)
            
            # 2. Spectral centroid (яркость)
            spectral_centroid = librosa.feature.spectral_centroid(y=self.y, sr=self.sr)[0]
            brightness = np.mean(spectral_centroid) / (self.sr / 2)
            
            # 3. Zero crossing rate (перкуссивность)
            zcr = librosa.feature.zero_crossing_rate(self.y)[0]
            percussiveness = np.mean(zcr)
            
            # 4. Spectral rolloff (частотная энергия)
            rolloff = librosa.feature.spectral_rolloff(y=self.y, sr=self.sr, roll_percent=0.85)[0]
            high_freq_energy = np.mean(rolloff) / (self.sr / 2)
            
            # Комбинируем метрики с весами
            energy = (
                rms_energy * 0.35 +
                brightness * 0.25 +
                percussiveness * 0.20 +
                high_freq_energy * 0.20
            )
            
            # Нормализуем к 0-1
            energy_normalized = min(1.0, max(0.0, energy / 0.12))
            
            # Определяем настроение (можно будет откалибровать позже)
            if energy_normalized < 0.3:
                mood = "Calm/Chill"
            elif energy_normalized < 0.5:
                mood = "Groovy/Smooth"
            elif energy_normalized < 0.65:
                mood = "Energetic/Danceable"
            elif energy_normalized < 0.82:
                mood = "Driving/Powerful"
            else:
                mood = "Peak/Intense"
            
            return round(energy_normalized, 3), mood
            
        except Exception as e:
            logger.error(f"Ошибка анализа энергии для {self.filename}: {e}")
            return 0.5, "Unknown"
    
    def analyze_structure(self) -> Dict[str, float]:
        """Анализ структуры трека для cue-точек с квантованием по битам"""
        if not self.loaded:
            return {'intro': 0.0, 'buildup': 0.0, 'drop': 0.0, 'outro': 0.0, 'duration': 0.0}
        
        try:
            # Получаем биты (для квантования)
            tempo, beats = librosa.beat.beat_track(y=self.y, sr=self.sr, units='time')
            if beats is None or len(beats) == 0:
                # fallback
                beats = []
            
            # Улучшенная сегментация
            mfcc = librosa.feature.mfcc(y=self.y, sr=self.sr, n_mfcc=20)
            boundaries_frames = librosa.segment.agglomerative(mfcc, k=6)
            boundaries = librosa.frames_to_time(boundaries_frames, sr=self.sr)
            duration = librosa.get_duration(y=self.y, sr=self.sr)
            
            # Определяем предварительные точки
            if len(boundaries) >= 4:
                raw_buildup = boundaries[1]
                raw_drop = boundaries[2]
                raw_outro = boundaries[-2] if len(boundaries) > 3 else duration * 0.85
            else:
                raw_buildup = duration * 0.25
                raw_drop = duration * 0.5
                raw_outro = duration * 0.85
            
            # Квантуем к ближайшему биту
            buildup = self._quantize_to_beat(raw_buildup, beats)
            drop = self._quantize_to_beat(raw_drop, beats)
            outro = self._quantize_to_beat(raw_outro, beats)
            
            # Убедимся, что порядок сохраняется
            if buildup > drop:
                buildup = drop - (beats[1]-beats[0])/2 if len(beats)>1 else drop - 0.5
            if drop > outro:
                drop = outro - (beats[1]-beats[0])/2 if len(beats)>1 else outro - 0.5
            
            structure = {
                'intro': 0.0,
                'buildup': buildup,
                'drop': drop,
                'outro': outro,
                'duration': duration
            }
            return structure
            
        except Exception as e:
            logger.error(f"Ошибка анализа структуры для {self.filename}: {e}")
            duration = librosa.get_duration(y=self.y, sr=self.sr) if self.y is not None else 0
            return {
                'intro': 0.0,
                'buildup': duration * 0.25,
                'drop': duration * 0.5,
                'outro': duration * 0.85,
                'duration': duration
            }
            
        except Exception as e:
            logger.error(f"Ошибка анализа структуры для {self.filename}: {e}")
            duration = librosa.get_duration(y=self.y, sr=self.sr) if self.y is not None else 0
            return {
                'intro': 0.0,
                'buildup': duration * 0.25,
                'drop': duration * 0.5,
                'outro': duration * 0.85,
                'duration': duration
            }
    
    def get_metadata(self) -> Dict[str, str]:
        """Извлечение существующих метаданных"""
        try:
            audio = File(self.file_path)
            
            metadata = {
                'artist': '',
                'title': '',
                'album': '',
                'genre': '',
                'year': '',
                'duration': 0
            }
            
            if audio is not None and hasattr(audio, 'tags') and audio.tags:
                tags = audio.tags
                
                if isinstance(tags, ID3):
                    metadata['artist'] = str(tags.get('TPE1', ''))
                    metadata['title'] = str(tags.get('TIT2', ''))
                    metadata['album'] = str(tags.get('TALB', ''))
                    metadata['genre'] = str(tags.get('TCON', ''))
                    metadata['year'] = str(tags.get('TDRC', ''))
                else:
                    metadata['artist'] = tags.get('artist', [''])[0] if 'artist' in tags else ''
                    metadata['title'] = tags.get('title', [''])[0] if 'title' in tags else ''
                    metadata['album'] = tags.get('album', [''])[0] if 'album' in tags else ''
                    metadata['genre'] = tags.get('genre', [''])[0] if 'genre' in tags else ''
                
                if hasattr(audio, 'info') and audio.info:
                    metadata['duration'] = int(audio.info.length)
            
            return metadata
            
        except Exception as e:
            logger.error(f"Ошибка чтения метаданных для {self.filename}: {e}")
            return {'artist': '', 'title': '', 'album': '', 'genre': '', 'year': '', 'duration': 0}
    
    def full_analysis(self) -> Optional[Dict]:
        """Полный анализ трека с проверкой загрузки"""
        if not self.load_audio():
            return None
        
        logger.info(f"Анализирую: {self.filename}")
        
        metadata = self.get_metadata()
        bpm = self.analyze_bpm()
        key, camelot = self.analyze_key()
        energy, mood = self.analyze_energy_mood()
        structure = self.analyze_structure()
        
        # Предварительные звёздочки (будут откалиброваны позже)
        if energy < 0.3:
            stars = 1
        elif energy < 0.5:
            stars = 2
        elif energy < 0.65:
            stars = 3
        elif energy < 0.82:
            stars = 4
        else:
            stars = 5
        
        result = {
            'file_path': self.file_path,
            'filename': self.filename,
            'artist': metadata['artist'],
            'title': metadata['title'],
            'album': metadata['album'],
            'existing_genre': metadata['genre'],
            'year': metadata['year'],
            'duration': structure['duration'],
            'bpm': bpm,
            'key': key,
            'camelot': camelot,
            'energy': energy,
            'mood': mood,
            'energy_stars': stars,
            'cue_intro': 0.0,
            'cue_buildup': structure.get('buildup', 0.0),
            'cue_drop': structure.get('drop', 0.0),
            'cue_outro': structure.get('outro', 0.0),
            'loop_start': 0.0,  # будет переопределено позже
            'loop_end': structure['duration']
        }
        
        return result


def analyze_track(file_path: str) -> Optional[Dict]:
    """Анализ одного трека"""
    analyzer = MusicAnalyzer(file_path)
    return analyzer.full_analysis()
